This files were based on scripts of 
https://issues.apache.org/jira/browse/HADOOP-6342 and https://issues.apache.org/jira/browse/HADOOP-6846

All instructions are made for pseudo-distributed operation (for hadoop running on a single-node in a pseudo-distributed mode where each hadoop daemon runs in a separate java process).

For installation:

- go to installation directory of Generate-Hadoop-Tarball
  cd Generate-Hadoop-Tarball
- all shell files with execution permission
  chmod +x *.sh



For compile, install and test facebook hadoop 0.20.1 version:

- create a installation directory, for example, /opt
  mkdir /opt

- go to installation directory
  cd /opt

- create a clone
  git clone https://github.com/facebook/hadoop-20-warehouse.git

- compile
  cd /opt/hadoop-20-warehouse
  ant -Dresolvers=internal -Djava5.home=/usr/lib/jvm/jdk1.5.0_22 -Dforrest.home=/opt/apache-forrest-0.9 -Djavac.args="-Xlint -Xmaxwarns 3000" binary > ant.log 2>&1

- if ok, there is a tarball in build dir
  ls -lagt /opt/hadoop-20-warehouse/build/hadoop-0.20.1-dev-bin.tar.gz

- copy generated tarball to installation directory 
  cp /opt/hadoop-20-warehouse/build/hadoop-0.20.1-dev-bin.tar.gz /opt

- go to installation directory
  cd /opt

- install, test hadoop and stop
 ./Generate-Hadoop-Tarball/run-pseudo-distributed-munged.sh 0.20.1 /opt



For install and test hadoop 0.21.0 version:

- create a installation directory, for example, /opt
  mkdir /opt

- go to installation directory
  cd /opt

- Download tarball from http://www.apache.org/dist/hadoop/core/
  ls -lagt /opt/hadoop-0.21.0.tar.gz

- install, test hadoop and stop
 ./Generate-Hadoop-Tarball/run-pseudo-distributed-munged.sh 0.21.0 /opt



For compile, install and test facebook hadoop 0.21.1 and 0.22.0 version (use 0.21.1 or 0.22.0 when applied)

- create a installation directory, for example, /opt
  mkdir /opt

- go to installation directory
  cd /opt

- be sure if common, hdfs and mapreduce do not exist
  rm -rf /opt/hadoop-common
  rm -rf /opt/hadoop-hdfs
  rm -rf /opt/hadoop-mapreduce

- make a checkout of hadoop svn repositories
  ./Generate-Hadoop-Tarball/checkout.sh 0.21.1 /opt

- verify /opt/check-{common,hdfs,mapreduce}.out
  tail /opt/check-{common,hdfs,mapreduce}.out

- go to common installation directory
  cd /opt/hadoop-common

- apply patch HADOOP-6342.patch https://issues.apache.org/jira/browse/HADOOP-6342
  patch -p0 < HADOOP-6342.patch

- copy combine-bindirs.sh of Generate-Hadoop-Tarball to combine-bindirs.sh
  cp Generate-Hadoop-Tarball/combine-bindirs.sh src/buildscripts/

- all shell files with execution permission
  chmod +x src/buildscripts/*.sh

- go to installation directory of Generate-Hadoop-Tarball
  cd Generate-Hadoop-Tarball

- generate tarball for first compilation
  ./generate-tarball.sh 0.21.1 /opt

- or regenerate tarball for others compilations
  ./generate-tarball-recompile.sh 0.21.1 /opt

- if ok then there is a tarball in /opt/hadoop-common/build/
  ls -lagt /opt/hadoop-common/build/hadoop-0.21.1.tar.gz

- copy generated tarball to installation directory 
  cp /opt/hadoop-common/build/hadoop-0.21.1.tar.gz /opt

- go to hadoop installation directory
  cd /opt

- install, test hadoop and stop
  ./Generate-Hadoop-Tarball/run-pseudo-distributed-munged.sh 0.21.1 /opt


For configure raid for facebook hadoop 0.20.1, 0.21.0, 0.21.1 and 0.22.0 versions: (use 0.20.1, 0.21.0, 0.21.1 and 0.22.0 when applied)

- go to installation directory of Generate-Hadoop-Tarball
  cd Generate-Hadoop-Tarball

- edit raid.xml with correct srcPath prefix and erasure code type (xor or rs)

- configure raid
 ./conf-raid.sh 0.20.1 /opt

- edit hdfs-raid-site.xml with correct paths and others parameters

- copy new hdfs-site.xml 
  example for 0.20.1: cp Generate-Hadoop-Tarball/conf/hdfs-raid-site.xml /opt/test/hadoop-0.20.1-dev/conf/hdfs-site.xml
  example for 0.21.0: cp Generate-Hadoop-Tarball/conf/hdfs-raid-site.xml /opt/test/hadoop-0.21.0/conf/hdfs-site.xml

- test new configurations
  start-dfs.sh
  start-mapred.sh # for 0.20.1 version, it starts raidnode too
  start-raidnode.sh # not necessary for facebook hadoop 0.20.1 version
  hadoop org.apache.hadoop.raid.RaidShell -showConfig # raid configuration

- copy to hdfs at least one file with more than 128MB of size (2 blocks of 64MB)
  an example: ubuntu-10.10-netbook-i386.iso file has about 700MB
  hadoop fs -copyToLocal ubuntu-10.10-netbook-i386.iso input/ubuntu-10.10-netbook-i386.iso

- stop test
  stop-raidnode.sh # not necessary for facebook hadoop 0.20.1 version
  stop-mapred.sh # for 0.20.1 version, it stops raidnode too
  stop-dfs.sh
