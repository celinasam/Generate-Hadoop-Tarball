This files were based on scripts of 
https://issues.apache.org/jira/browse/HADOOP-6342 and https://issues.apache.org/jira/browse/HADOOP-6846

All instructions are made for pseudo-distributed operation (for hadoop running on a single-node in a pseudo-distributed mode where each hadoop daemon runs in a separate java process).

For download Generated-Hadoop-Tarball:

- git clone https://github.com/celinasam/Generate-Hadoop-Tarball



For install and test hadoop 0.20.203.0, 0.21.0, 0.21.1 and 0.22.0 versions: (use 0.20.203.0, 0.21.0, 0.21.1 and 0.22.0 when applied)

- create a installation directory, for example, /opt
  mkdir /opt

- go to installation directory
  cd /opt

- Download tarball from http://www.apache.org/dist/hadoop/core/ for 0.20.203.0 and 0.21.0 versions
- Generate tarball using next instructions for 0.21.1 and 0.22.0 versions
  ls -lagt /opt/hadoop-0.21.0.tar.gz

- install, test hadoop and stop
  Generate-Hadoop-Tarball/run-pseudo-distributed-munged.sh 0.21.0 /opt

  answer Y for "Re-format filesystem in ... ? (Y or N)"



For compile, install and test 0.21.1 (branch 0.21) and 0.22.0 versions (branch 0.22) (use 0.21.1 or 0.22.0 when applied) from http://svn.apache.org/viewvc/hadoop/common/branches/

- create a installation directory, for example, /opt
  mkdir /opt

- go to installation directory
  cd /opt

- be sure if common, hdfs and mapreduce do not exist
  rm -rf /opt/hadoop-common
  rm -rf /opt/hadoop-hdfs
  rm -rf /opt/hadoop-mapreduce

- make a checkout of hadoop svn repositories
  Generate-Hadoop-Tarball/checkout.sh 0.21.1 /opt

- verify /opt/check-{common,hdfs,mapreduce}.out
  tail /opt/check-{common,hdfs,mapreduce}.out

- go to common installation directory
  cd /opt/hadoop-common

- apply patch HADOOP-6342.patch https://issues.apache.org/jira/browse/HADOOP-6342
  patch -p0 < HADOOP-6342.patch

- copy combine-bindirs.sh of Generate-Hadoop-Tarball to combine-bindirs.sh
  cp Generate-Hadoop-Tarball/combine-bindirs.sh src/buildscripts/

- all shell files with execution permission
  chmod +x src/buildscripts/*.sh

- you can do modifications on source code

- go to installation directory of Generate-Hadoop-Tarball
  cd Generate-Hadoop-Tarball

- generate tarball for first compilation
  ./generate-tarball.sh 0.21.1 /opt

- or regenerate tarball for others compilations
  ./generate-tarball-recompile.sh 0.21.1 /opt

- if ok then there is a tarball in /opt/hadoop-common/build/
  ls -lagt /opt/hadoop-common/build/hadoop-0.21.1.tar.gz

- copy generated tarball to installation directory 
  cp /opt/hadoop-common/build/hadoop-0.21.1.tar.gz /opt

- go to hadoop installation directory
  cd /opt

- install, test hadoop and stop
  Generate-Hadoop-Tarball/run-pseudo-distributed-munged.sh 0.21.1 /opt


For configure and test raid for 0.21.0, 0.21.1 and 0.22.0 versions: (use 0.21.0, 0.21.1 and 0.22.0 when applied)

- go to installation directory of Generate-Hadoop-Tarball
  cd Generate-Hadoop-Tarball

- edit conf/raid.xml with correct srcPath prefix and erasure code type (xor for 0.21.*; xor or rs for 0.22.0)

- configure raid
  ./conf-raid.sh 0.21.0 /opt

- edit conf/hdfs-raid-site.xml with correct paths and others parameters

- copy new hdfs-site.xml 
  cp conf/hdfs-raid-site.xml /opt/test/hadoop-0.21.0/conf/hdfs-site.xml

- edit hadoop-bashrc.sh with correct paths and save like hadoop-bashrc-<version>.sh (use 0.21.0, 0.21.1 and 0.22.0)

- define environment variables
  . hadoop-bashrc-<version>.sh

- test new configurations
  start-dfs.sh
  start-mapred.sh

- see info about namenode in your browser. you can continue if safe mode is off.
  http://localhost:50070

- copy to hdfs at least one file with more than 128MB of size (2 blocks of 64MB)
  an example: ubuntu-10.10-netbook-i386.iso file has about 700MB
  hadoop fs -copyFromLocal ubuntu-10.10-netbook-i386.iso input

- start raid node
  start-raidnode.sh

- show raid configuration (appears not working for 0.21.*; only for 0.22.0)
  hadoop org.apache.hadoop.raid.RaidShell -showConfig

- see info about namenode in your browser
  http://localhost:50070
  
  about task tracker
  http://localhost:50060

  about job tracker
  http://localhost:50030

- stop test
  stop-raidnode.sh
  stop-mapred.sh
  stop-dfs.sh
